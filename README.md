# ğŸš€ Distributed PyTorch Multi-Node GPU Training

**Production-ready multi-node distributed PyTorch training setup using WSL2, NCCL, and torchrun.**

Scale your deep learning workloads across multiple Windows machines with GPUs, leveraging NVIDIA's NCCL for high-performance GPU-to-GPU communication.

## ğŸ¯ Overview

This project provides a complete, battle-tested solution for distributed PyTorch training across multiple physical machines:

- **ğŸ–¥ï¸ Platform**: Windows 11 + WSL2 (Ubuntu)
- **âš¡ Backend**: NCCL (GPU-accelerated) + Gloo (CPU fallback)
- **ğŸ”— Networking**: WSL2 Mirrored Networking Mode
- **ğŸ® GPUs**: NVIDIA CUDA-enabled GPUs (RTX/GTX series)
- **ğŸ“¡ Rendezvous**: PyTorch's native distributed launcher (`torchrun`)

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Node 01 (Master - Rank 0)     â”‚      â”‚   Node 02 (Worker - Rank 1)     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Windows 11 + WSL2       â”‚   â”‚      â”‚  â”‚  Windows 11 + WSL2       â”‚   â”‚
â”‚  â”‚  â”œâ”€ Ubuntu 22.04         â”‚   â”‚      â”‚  â”‚  â”œâ”€ Ubuntu 22.04         â”‚   â”‚
â”‚  â”‚  â”œâ”€ PyTorch 2.x + CUDA   â”‚   â”‚      â”‚  â”‚  â”œâ”€ PyTorch 2.x + CUDA   â”‚   â”‚
â”‚  â”‚  â”œâ”€ NCCL 2.21.5          â”‚   â”‚      â”‚  â”‚  â”œâ”€ NCCL 2.21.5          â”‚   â”‚
â”‚  â”‚  â””â”€ NVIDIA GPU (CUDA)    â”‚   â”‚      â”‚  â”‚  â””â”€ NVIDIA GPU (CUDA)    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚         192.168.29.67            â”‚      â”‚         192.168.29.197          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚                                    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                              Local Network / Direct Ethernet
                              (10-50 GB/s with NCCL)
```

### Key Features

âœ… **Multi-node NCCL**: True GPU-to-GPU communication across physical machines  
âœ… **WSL2 Native**: No Docker, no VM overhead - direct CUDA access  
âœ… **Mirrored Networking**: WSL2's latest networking mode for seamless connectivity  
âœ… **Dual Backend**: NCCL for performance, Gloo for compatibility  
âœ… **Production Ready**: Comprehensive troubleshooting and firewall configuration  
âœ… **Easy Setup**: Step-by-step guides for both master and worker nodes

## ğŸ“ Project Structure

```
dist-gpu-windows/
â”œâ”€â”€ ğŸ“„ train_torchrun.py           # Main training script (NCCL backend)
â”œâ”€â”€ ğŸ“„ worker.py                   # Standalone worker script
â”œâ”€â”€ ğŸ“„ requirements.txt            # Python dependencies
â”‚
â”œâ”€â”€ ğŸ“š Documentation
â”‚   â”œâ”€â”€ NODE01_MASTER.md           # Complete Node 01 setup guide
â”‚   â”œâ”€â”€ NODE02_WORKER.md           # Complete Node 02 setup guide
â”‚   â”œâ”€â”€ QUICK_START.md             # Quick reference guide
â”‚   â”œâ”€â”€ SOLUTION.md                # Architecture & design decisions
â”‚   â”œâ”€â”€ WSL_MIRRORED_NETWORKING.md # Mirrored networking setup
â”‚   â””â”€â”€ WINDOWS_SETUP.md           # Windows-specific instructions
â”‚
â”œâ”€â”€ ğŸš€ Launch Scripts
â”‚   â”œâ”€â”€ run_node0.sh               # Launch master node
â”‚   â”œâ”€â”€ run_node1.sh               # Launch worker node
â”‚   â”œâ”€â”€ run_single_node.sh         # Single-node testing
â”‚   â””â”€â”€ run_train.sh               # Training launcher
â”‚
â”œâ”€â”€ ğŸ“¦ Archive & Tests
â”‚   â”œâ”€â”€ archive/                   # Previous implementations
â”‚   â”œâ”€â”€ Test/                      # Test scripts and utilities
â”‚   â””â”€â”€ issues/                    # Issue tracking and solutions
â”‚
â””â”€â”€ ğŸ”¬ Experimental
    â””â”€â”€ L1-nbdistributed/          # Jupyter notebook experiments
```

### Key Files

| File | Purpose | When to Use |
|------|---------|-------------|
| `train_torchrun.py` | Main distributed training script with NCCL | Multi-node GPU training (recommended) |
| `NODE01_MASTER.md` | Master node setup instructions | Setting up Node 01 |
| `NODE02_WORKER.md` | Worker node setup instructions | Setting up Node 02 |
| `QUICK_START.md` | Quick reference and troubleshooting | Fast setup & debugging |
| `WSL_MIRRORED_NETWORKING.md` | WSL2 networking configuration | Enabling mirrored mode |

## ğŸš€ Quick Start

### Prerequisites

**System Requirements (Both Nodes):**
- ğŸ’» Windows 11 (Build 22621+ for mirrored networking)
- ğŸ§ WSL2 with Ubuntu 22.04
- ğŸ® NVIDIA GPU (RTX/GTX series)
- ğŸ”§ NVIDIA CUDA on WSL driver
- ğŸŒ Same local network or direct Ethernet connection

**Software Requirements:**
- Python 3.10+
- PyTorch 2.x with CUDA 12.4 support
- Git

---

### ğŸ¯ 30-Second Setup

**On Both Node 01 and Node 02:**

#### 1ï¸âƒ£ Install WSL2 (Windows PowerShell as Admin)
```powershell
wsl --install -d Ubuntu-22.04
```

#### 2ï¸âƒ£ Clone Repository (Inside WSL)
```bash
git clone <your-repo-url> ~/dist-gpu-windows
cd ~/dist-gpu-windows
```

#### 3ï¸âƒ£ Setup Python Environment (WSL)
```bash
# Install dependencies
sudo apt update && sudo apt install -y build-essential python3 python3-pip python3-venv git

# Create virtual environment
python3 -m venv .venv
source .venv/bin/activate

# Install PyTorch with CUDA
pip install --upgrade pip setuptools wheel
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
```

#### 4ï¸âƒ£ Enable WSL2 Mirrored Networking (Windows PowerShell as Admin)
```powershell
# Create .wslconfig
@"
[wsl2]
networkingMode=mirrored
"@ | Out-File -FilePath "$env:USERPROFILE\.wslconfig" -Encoding ASCII -Force

# Restart WSL
wsl --shutdown
Start-Sleep -Seconds 30
wsl
```

#### 5ï¸âƒ£ Configure Firewall (Windows PowerShell as Admin)
```powershell
# Disable Windows Firewall for Private network (recommended for testing)
Set-NetFirewallProfile -Profile Private -Enabled False

# OR add specific rules
New-NetFirewallRule -DisplayName "PyTorch Distributed 29500" -Direction Inbound -LocalPort 29500 -Protocol TCP -Action Allow
New-NetFirewallRule -DisplayName "NCCL Communication Ports" -Direction Inbound -LocalPort 20000-40000 -Protocol TCP -Action Allow
```

#### 6ï¸âƒ£ Disable Antivirus (Temporarily)
- **Norton/Avira**: Disable firewall during testing
- This prevents blocking of NCCL GPU communication

---

### ğŸƒ Run Multi-Node Training

**Step 1: Get Node 01's IP (WSL on Node 01)**
```bash
ip addr show eth0 | grep "inet "
# Example: 192.168.29.67
```

**Step 2: Start Master Node (WSL on Node 01)**
```bash
cd ~/dist-gpu-windows
source .venv/bin/activate

# Set NCCL environment variables
export NCCL_IB_DISABLE=1
export NCCL_P2P_DISABLE=1
export NCCL_SOCKET_IFNAME=eth0
export NCCL_DEBUG=INFO

# Launch master (replace IP with your Node 01 IP)
torchrun --nnodes=2 --node_rank=0 --nproc_per_node=1 \
  --master_addr=192.168.29.67 \
  --master_port=29500 \
  train_torchrun.py
```

**Step 3: Start Worker Node (WSL on Node 02)**
```bash
cd ~/dist-gpu-windows
source .venv/bin/activate

# Set NCCL environment variables
export NCCL_IB_DISABLE=1
export NCCL_P2P_DISABLE=1
export NCCL_SOCKET_IFNAME=eth0
export NCCL_DEBUG=INFO

# Launch worker (use Node 01's IP)
torchrun --nnodes=2 --node_rank=1 --nproc_per_node=1 \
  --master_addr=192.168.29.67 \
  --master_port=29500 \
  train_torchrun.py
```

### âœ… Expected Output

**Both nodes should show:**
```
Initializing with backend: nccl
[rank X] world_size=2 device=cuda hostname=NODE-NAME
NCCL INFO Bootstrap : Using eth0:192.168.29.XX<0>
NCCL INFO Connected all rings
NCCL INFO Connected all trees
[rank X] gathered=[0, 1]
[rank X] barrier OK; shutting down
```

---

### ğŸ“š Detailed Guides

For complete setup instructions and troubleshooting:

| Guide | Description |
|-------|-------------|
| ğŸ“– [NODE01_MASTER.md](NODE01_MASTER.md) | Complete setup for master node (Rank 0) |
| ğŸ“– [NODE02_WORKER.md](NODE02_WORKER.md) | Complete setup for worker node (Rank 1) |
| ğŸ“– [QUICK_START.md](QUICK_START.md) | Quick reference and common commands |
| ğŸ“– [WSL_MIRRORED_NETWORKING.md](WSL_MIRRORED_NETWORKING.md) | WSL2 networking configuration |
| ğŸ“– [SOLUTION.md](SOLUTION.md) | Architecture and design decisions |

## ğŸ”§ Configuration

### Network Settings

| Setting | Value | Description |
|---------|-------|-------------|
| **Port** | 29500 | Master rendezvous port |
| **Backend** | NCCL / Gloo | NCCL for GPU, Gloo for CPU fallback |
| **Rendezvous** | Native `torchrun` | Uses `--master_addr` and `--master_port` |
| **Network Mode** | WSL2 Mirrored | Direct host network access |
| **Communication** | TCP/IP | Over Ethernet (eth0) |

### NCCL Environment Variables

```bash
export NCCL_IB_DISABLE=1         # Disable InfiniBand (not available on consumer hardware)
export NCCL_P2P_DISABLE=1        # Disable peer-to-peer GPU access (for multi-node)
export NCCL_SOCKET_IFNAME=eth0   # Use Ethernet interface
export NCCL_DEBUG=INFO           # Enable debug logging
```

### Environment Variables (Auto-set by torchrun)

```bash
MASTER_ADDR=192.168.29.67   # Set via --master_addr
MASTER_PORT=29500           # Set via --master_port
WORLD_SIZE=2                # Total number of processes
RANK=0/1                    # Process rank (0=master, 1=worker)
LOCAL_RANK=0                # GPU index on local machine
```

## ğŸ§ª Testing & Validation

### Verify CUDA and NCCL (WSL)
```bash
python -c "import torch; print('PyTorch:', torch.__version__); print('CUDA:', torch.cuda.is_available()); print('NCCL:', torch.distributed.is_nccl_available())"
```

### Test Network Connectivity
```bash
# From Node 02 to Node 01
ping 192.168.29.67

# Check if port is listening (on Node 01 after starting master)
ss -tulpn | grep 29500
```

### Verify WSL Mirrored Networking
```bash
# Your WSL IP should match Windows IP
ip addr show eth0 | grep "inet "
```

### Single-Node Test (Before Multi-Node)
```bash
# Test on one machine with 1 GPU
torchrun --nproc_per_node=1 --nnodes=1 train_torchrun.py
```

## ğŸ“Š Performance

### NCCL Backend (Recommended)
- **Bandwidth**: 10-50 GB/s (GPU-to-GPU direct)
- **Latency**: < 10Î¼s (local network)
- **Use Case**: Production training, large models
- **Requirements**: WSL2 mirrored networking

### Gloo Backend (Fallback)
- **Bandwidth**: 1-10 GB/s (CPU-mediated)
- **Latency**: ~100Î¼s
- **Use Case**: Development, compatibility testing
- **Requirements**: Standard WSL2 NAT networking

### Network Recommendations
- âœ… **Best**: Direct Ethernet cable (10 Gbps)
- âœ… **Good**: WiFi 6 on same router (1-2 Gbps)
- âš ï¸ **Avoid**: WiFi with AP isolation enabled

## ğŸ› Troubleshooting

### Common Issues & Solutions

#### âŒ Ping Fails Between Nodes
**Symptoms:** `ping 192.168.29.67` times out

**Causes & Fixes:**
1. **Router AP Isolation**: 
   - Access router admin (`http://192.168.29.1` for Jio Fiber)
   - Disable "AP Isolation" or "Client Isolation"
   - Reboot router

2. **Windows Firewall**:
   ```powershell
   Set-NetFirewallProfile -Profile Private -Enabled False
   ```

3. **Antivirus Blocking** (Norton/Avira):
   - Temporarily disable antivirus firewall
   - Add exceptions for Python and ports 20000-40000

4. **Alternative**: Use direct Ethernet cable between laptops

#### âŒ NCCL Connection Timeout
**Error:** `The client socket has timed out after 60000ms`

**Solutions:**
1. Ensure both nodes have WSL2 mirrored networking enabled
2. Verify firewall is disabled: `Get-NetFirewallProfile`
3. Check NCCL environment variables are set on both nodes
4. Use `--master_addr` and `--master_port` (not `--rdzv_endpoint`)

#### âŒ NCCL Connection Reset / Socket Error
**Error:** `socketStartConnect: Connect to IP<port> failed : Software caused connection abort`

**Solutions:**
1. Disable Windows Firewall on both nodes (see Step 5)
2. Disable antivirus (Norton, Avira, etc.)
3. Verify mirrored networking: `ip addr show eth0 | grep "inet "`
4. Add firewall rules for ports 20000-40000

#### âŒ CUDA Not Available
**Error:** `CUDA: False` when checking PyTorch

**Solutions:**
1. Install NVIDIA CUDA on WSL driver from [nvidia.com/cuda/wsl](https://developer.nvidia.com/cuda/wsl)
2. Verify with: `nvidia-smi` (should show GPU)
3. Reinstall PyTorch with CUDA:
   ```bash
   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
   ```

#### âŒ NCCL Not Available
**Error:** `NCCL: False` when checking PyTorch

**Solution:** Install PyTorch with CUDA support (NCCL is included)

#### âŒ WSL Mirrored Networking Not Working
**Error:** WSL IP still shows `172.x.x.x` instead of Windows IP

**Solutions:**
1. Verify Windows version: Build 22621+ required
   ```powershell
   [System.Environment]::OSVersion.Version
   ```
2. Check `.wslconfig` location: `C:\Users\<YourUser>\.wslconfig`
3. Ensure file has correct format (no BOM, ASCII encoding)
4. Full WSL restart:
   ```powershell
   wsl --shutdown
   Start-Sleep -Seconds 30
   wsl
   ```

#### âŒ Port Already in Use
**Error:** `OSError: [Errno 98] Address already in use`

**Solutions:**
1. Find process using port: `sudo lsof -i :29500`
2. Kill the process: `sudo kill -9 <PID>`
3. Or use a different port (e.g., 29501)

### Network Requirements Checklist

- âœ… Both machines on same WiFi network or direct Ethernet
- âœ… Windows Firewall disabled for Private network
- âœ… Antivirus firewall disabled (Norton, Avira, etc.)
- âœ… WSL2 mirrored networking enabled (Build 22621+)
- âœ… Port 29500 accessible (rendezvous)
- âœ… Ports 20000-40000 accessible (NCCL communication)
- âœ… No VPN or proxy interference
- âœ… Router AP Isolation disabled

## ğŸ”„ Development Workflow

1. **Start Simple**: Test single-node first, then multi-node
2. **Enable Mirrored Networking**: Critical for NCCL to work
3. **Disable Firewalls**: Start with all firewalls off, add rules later
4. **Check Connectivity**: Ensure nodes can ping each other
5. **Monitor Logs**: Use `NCCL_DEBUG=INFO` to diagnose issues
6. **Scale Gradually**: 2 nodes â†’ 3 nodes â†’ N nodes

## ğŸ“ˆ Performance Tips

### Optimize Network
- âœ… Use direct Ethernet connection for lowest latency
- âœ… Disable power-saving on network adapters
- âœ… Use dedicated network interface for distributed training
- âœ… Monitor bandwidth: `iperf3` between nodes

### Optimize Training
- ğŸ¯ Batch size: Larger batches better utilize multi-GPU
- ğŸ¯ Gradient accumulation: Simulate larger batches
- ğŸ¯ Mixed precision: Use FP16 to reduce communication overhead
- ğŸ¯ Efficient collectives: Use `all_reduce` over `all_gather` when possible

### Monitor Performance
```bash
# GPU utilization
nvidia-smi -l 1

# Network traffic
iftop -i eth0

# NCCL performance test
nccl-tests/build/all_reduce_perf -b 8 -e 128M -f 2 -g 1
```

## ğŸ¤ Contributing

Contributions are welcome! This project is the result of extensive troubleshooting and experimentation with WSL2 + NCCL multi-node setups.

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-improvement`)
3. Test on both master and worker nodes
4. Commit your changes (`git commit -m 'Add amazing improvement'`)
5. Push to the branch (`git push origin feature/amazing-improvement`)
6. Open a Pull Request

## ğŸ“ License

MIT License - feel free to use this in your projects!

## ğŸ™ Acknowledgments

- [PyTorch Distributed](https://pytorch.org/tutorials/beginner/dist_overview.html) - Official documentation
- [NVIDIA NCCL](https://developer.nvidia.com/nccl) - High-performance GPU communication
- [WSL2 Mirrored Networking](https://learn.microsoft.com/en-us/windows/wsl/networking) - Microsoft WSL docs
- Community contributions and testing

---

## ğŸ’ Made with Love

**Created by [Amardeep Singh Sidhu](https://github.com/thefirehacker) ([@thefirehacker](https://github.com/thefirehacker))**

*Building AI solutions at [@AIEdX](https://github.com/AIEdX) & [@bubblspace](https://github.com/bubblspace)*

ğŸŒ [bubblspace.com](https://bubblspace.com) | ğŸ¦ [@thefirehacker](https://twitter.com/thefirehacker) | ğŸ’¼ [LinkedIn](https://linkedin.com/in/thefirehacker)

---

â­ If this project helped you, consider giving it a star on GitHub!

ğŸ“§ Questions? Open an issue or reach out at contact@aiedx.com
