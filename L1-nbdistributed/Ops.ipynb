{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa9af310",
   "metadata": {},
   "source": [
    "# Distributed PyTorch Operations with nbdistributed\n",
    "\n",
    "This notebook demonstrates distributed all_gather operations across multiple devices using nbdistributed package.\n",
    "\n",
    "## Setup Overview\n",
    "- **Coordinator (Mac)**: Runs this notebook and coordinates operations\n",
    "- **Worker (Windows RTX 2080)**: Provides GPU computation power\n",
    "\n",
    "## Requirements\n",
    "- Both machines must be on the same network\n",
    "- Windows machine should have CUDA-enabled PyTorch installed\n",
    "- nbdistributed package installed on both machines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cecf1894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nbdistributed in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (0.1.0)\n",
      "Requirement already satisfied: torch in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (2.8.0)\n",
      "Requirement already satisfied: torchvision in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (0.23.0)\n",
      "Requirement already satisfied: ipython>=8.0.0 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from nbdistributed) (9.5.0)\n",
      "Requirement already satisfied: jupyter>=1.0.0 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from nbdistributed) (1.1.1)\n",
      "Requirement already satisfied: pyzmq>=24.0.0 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from nbdistributed) (27.1.0)\n",
      "Requirement already satisfied: filelock in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: numpy in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from torchvision) (2.3.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: decorator in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from ipython>=8.0.0->nbdistributed) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from ipython>=8.0.0->nbdistributed) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from ipython>=8.0.0->nbdistributed) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from ipython>=8.0.0->nbdistributed) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from ipython>=8.0.0->nbdistributed) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from ipython>=8.0.0->nbdistributed) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from ipython>=8.0.0->nbdistributed) (2.19.2)\n",
      "Requirement already satisfied: stack_data in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from ipython>=8.0.0->nbdistributed) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from ipython>=8.0.0->nbdistributed) (5.14.3)\n",
      "Requirement already satisfied: wcwidth in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=8.0.0->nbdistributed) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from jedi>=0.16->ipython>=8.0.0->nbdistributed) (0.8.5)\n",
      "Requirement already satisfied: notebook in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from jupyter>=1.0.0->nbdistributed) (7.4.5)\n",
      "Requirement already satisfied: jupyter-console in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from jupyter>=1.0.0->nbdistributed) (6.6.3)\n",
      "Requirement already satisfied: nbconvert in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from jupyter>=1.0.0->nbdistributed) (7.16.6)\n",
      "Requirement already satisfied: ipykernel in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from jupyter>=1.0.0->nbdistributed) (6.30.1)\n",
      "Requirement already satisfied: ipywidgets in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from jupyter>=1.0.0->nbdistributed) (8.1.7)\n",
      "Requirement already satisfied: jupyterlab in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from jupyter>=1.0.0->nbdistributed) (4.4.7)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from pexpect>4.3->ipython>=8.0.0->nbdistributed) (0.7.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: appnope>=0.1.2 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0->nbdistributed) (0.1.4)\n",
      "Requirement already satisfied: comm>=0.1.1 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0->nbdistributed) (0.2.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0->nbdistributed) (1.8.17)\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0->nbdistributed) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0->nbdistributed) (5.8.1)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0->nbdistributed) (1.6.0)\n",
      "Requirement already satisfied: packaging>=22 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0->nbdistributed) (25.0)\n",
      "Requirement already satisfied: psutil>=5.7 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0->nbdistributed) (7.1.0)\n",
      "Requirement already satisfied: tornado>=6.2 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0->nbdistributed) (6.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from jupyter-client>=8.0.0->ipykernel->jupyter>=1.0.0->nbdistributed) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter>=1.0.0->nbdistributed) (4.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->jupyter-client>=8.0.0->ipykernel->jupyter>=1.0.0->nbdistributed) (1.17.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from ipywidgets->jupyter>=1.0.0->nbdistributed) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from ipywidgets->jupyter>=1.0.0->nbdistributed) (3.0.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from jupyterlab->jupyter>=1.0.0->nbdistributed) (2.0.5)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from jupyterlab->jupyter>=1.0.0->nbdistributed) (0.28.1)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from jupyterlab->jupyter>=1.0.0->nbdistributed) (2.3.0)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from jupyterlab->jupyter>=1.0.0->nbdistributed) (2.17.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from jupyterlab->jupyter>=1.0.0->nbdistributed) (2.27.3)\n",
      "Requirement already satisfied: notebook-shim>=0.2 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from jupyterlab->jupyter>=1.0.0->nbdistributed) (0.2.4)\n",
      "Requirement already satisfied: setuptools>=41.1.0 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from jupyterlab->jupyter>=1.0.0->nbdistributed) (65.5.0)\n",
      "Requirement already satisfied: anyio in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter>=1.0.0->nbdistributed) (4.11.0)\n",
      "Requirement already satisfied: certifi in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter>=1.0.0->nbdistributed) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter>=1.0.0->nbdistributed) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter>=1.0.0->nbdistributed) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.25.0->jupyterlab->jupyter>=1.0.0->nbdistributed) (0.16.0)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->nbdistributed) (25.1.0)\n",
      "Requirement already satisfied: jupyter-events>=0.11.0 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->nbdistributed) (0.12.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->nbdistributed) (0.5.3)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->nbdistributed) (5.10.4)\n",
      "Requirement already satisfied: overrides>=5.0 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->nbdistributed) (7.7.0)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->nbdistributed) (0.23.1)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->nbdistributed) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->nbdistributed) (0.18.1)\n",
      "Requirement already satisfied: websocket-client>=1.7 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->nbdistributed) (1.8.0)\n",
      "Requirement already satisfied: babel>=2.10 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->nbdistributed) (2.17.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->nbdistributed) (0.12.1)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->nbdistributed) (4.25.1)\n",
      "Requirement already satisfied: requests>=2.31 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->nbdistributed) (2.32.5)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from anyio->httpx<1,>=0.25.0->jupyterlab->jupyter>=1.0.0->nbdistributed) (1.3.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->nbdistributed) (25.1.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->nbdistributed) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->nbdistributed) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->nbdistributed) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->nbdistributed) (0.27.1)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->nbdistributed) (3.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.3 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->nbdistributed) (6.0.2)\n",
      "Requirement already satisfied: rfc3339-validator in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->nbdistributed) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->nbdistributed) (0.1.1)\n",
      "Requirement already satisfied: fqdn in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->nbdistributed) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->nbdistributed) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->nbdistributed) (3.0.0)\n",
      "Requirement already satisfied: rfc3987-syntax>=1.1.0 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->nbdistributed) (1.1.0)\n",
      "Requirement already satisfied: uri-template in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->nbdistributed) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->nbdistributed) (24.11.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0->nbdistributed) (4.13.5)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->nbdistributed) (6.2.0)\n",
      "Requirement already satisfied: defusedxml in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0->nbdistributed) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0->nbdistributed) (0.3.0)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0->nbdistributed) (3.1.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0->nbdistributed) (0.10.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0->nbdistributed) (1.5.1)\n",
      "Requirement already satisfied: webencodings in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->nbdistributed) (0.5.1)\n",
      "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->nbdistributed) (1.4.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->nbdistributed) (2.21.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->nbdistributed) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->nbdistributed) (2.5.0)\n",
      "Requirement already satisfied: lark>=1.2.2 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->nbdistributed) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->nbdistributed) (2.0.0)\n",
      "Requirement already satisfied: pycparser in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->nbdistributed) (2.23)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from beautifulsoup4->nbconvert->jupyter>=1.0.0->nbdistributed) (2.8)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->nbdistributed) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->nbdistributed) (2.9.0.20250822)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from stack_data->ipython>=8.0.0->nbdistributed) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from stack_data->ipython>=8.0.0->nbdistributed) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /Users/booimac/AIEDX/Code/Code-Scratch/.venv/lib/python3.11/site-packages (from stack_data->ipython>=8.0.0->nbdistributed) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install nbdistributed torch torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7381c9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Copy this Mac IP for Windows: 192.168.29.234\n",
      "‚úÖ TCPStore up on 0.0.0.0:12355, expecting 2 workers\n",
      "Keep this cell running while workers start‚Ä¶ (Ctrl+C to stop)\n"
     ]
    }
   ],
   "source": [
    "# Controller-only: start rendezvous store (TCPStore) on the Mac\n",
    "# This does NOT join the training process group.\n",
    "from torch.distributed import TCPStore\n",
    "import socket, time\n",
    "\n",
    "# Detect this Mac's LAN IP for copy/paste on Windows\n",
    "def get_local_ip():\n",
    "    try:\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_DGRAM) as s:\n",
    "            s.connect((\"8.8.8.8\", 80))\n",
    "            return s.getsockname()[0]\n",
    "    except Exception:\n",
    "        return socket.gethostbyname(socket.gethostname())\n",
    "\n",
    "LOCAL_IP = get_local_ip()\n",
    "print(f\"üìã Copy this Mac IP for Windows: {LOCAL_IP}\")\n",
    "\n",
    "# Start TCPStore listening on all interfaces\n",
    "MAC_BIND = \"0.0.0.0\"\n",
    "PORT = 12355\n",
    "WORLD_SIZE = 2       # number of Windows worker processes that will join\n",
    "\n",
    "store = TCPStore(host_name=LOCAL_IP, port=PORT, world_size=WORLD_SIZE, is_master=True, use_libuv=False)\n",
    "print(f\"‚úÖ TCPStore up on {MAC_BIND}:{PORT}, expecting {WORLD_SIZE} workers\")\n",
    "print(\"Keep this cell running while workers start‚Ä¶ (Ctrl+C to stop)\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(3600)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"üõë TCPStore stopped\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd4cfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed imports and setup (removing problematic nbdistributed decorator)\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import os\n",
    "import socket\n",
    "import time\n",
    "\n",
    "# Check if CUDA is available on this machine (Mac)\n",
    "print(f\"CUDA available on Mac: {torch.cuda.is_available()}\")\n",
    "print(f\"Mac will run as CPU coordinator (Rank 0)\")\n",
    "\n",
    "# Check for Mac's Metal GPU support\n",
    "if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    print(\"Mac Metal Performance Shaders (MPS) available for local operations\")\n",
    "    mac_device = \"mps\"\n",
    "else:\n",
    "    print(\"Using CPU for Mac operations\")\n",
    "    mac_device = \"cpu\"\n",
    "\n",
    "print(f\"Mac device: {mac_device}\")\n",
    "\n",
    "# Get local IP address automatically from network interface\n",
    "def get_local_ip():\n",
    "    \"\"\"Get the actual local IP address (not localhost)\"\"\"\n",
    "    try:\n",
    "        # Connect to a remote address to determine which interface to use\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_DGRAM) as s:\n",
    "            # Connect to Google DNS (doesn't actually send data)\n",
    "            s.connect((\"8.8.8.8\", 80))\n",
    "            local_ip = s.getsockname()[0]\n",
    "        return local_ip\n",
    "    except Exception:\n",
    "        # Fallback method\n",
    "        import subprocess\n",
    "        try:\n",
    "            # Get IP from ifconfig (macOS/Linux)\n",
    "            result = subprocess.run(['ifconfig'], capture_output=True, text=True)\n",
    "            for line in result.stdout.split('\\n'):\n",
    "                if 'inet ' in line and '127.0.0.1' not in line and 'inet 169.254' not in line:\n",
    "                    ip = line.split('inet ')[1].split(' ')[0]\n",
    "                    if ip.startswith('192.168.') or ip.startswith('10.') or ip.startswith('172.'):\n",
    "                        return ip\n",
    "        except:\n",
    "            pass\n",
    "        return \"192.168.1.1\"  # Last resort fallback\n",
    "\n",
    "local_ip = get_local_ip()\n",
    "hostname = socket.gethostname()\n",
    "\n",
    "print(f\"üîç Auto-detected Mac IP (Master): {local_ip}\")\n",
    "print(f\"üì± Hostname: {hostname}\")\n",
    "print(f\"‚úÖ Using auto-detected IP address (not hostname)\")\n",
    "print(\"\\nSetup: Mac (CPU/MPS coordinator) + Windows (CUDA GPU worker)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e50c8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributed configuration\n",
    "MASTER_ADDR = local_ip  # This Mac will be the master\n",
    "MASTER_PORT = \"12355\"   # Port for communication\n",
    "WORLD_SIZE = 2          # Total number of processes (Mac + Windows)\n",
    "RANK = 0               # This machine's rank (master = 0)\n",
    "\n",
    "print(f\"Master Address (this Mac): {MASTER_ADDR}\")\n",
    "print(f\"Master Port: {MASTER_PORT}\")\n",
    "print(f\"World Size: {WORLD_SIZE}\")\n",
    "print(f\"This machine's rank: {RANK}\")\n",
    "\n",
    "# Set environment variables for distributed training\n",
    "os.environ['MASTER_ADDR'] = MASTER_ADDR\n",
    "os.environ['MASTER_PORT'] = MASTER_PORT\n",
    "os.environ['WORLD_SIZE'] = str(WORLD_SIZE)\n",
    "os.environ['RANK'] = str(RANK)\n",
    "\n",
    "print(f\"‚úÖ Mac configured as master node at {MASTER_ADDR}:{MASTER_PORT}\")\n",
    "print(f\"üîç Environment check:\")\n",
    "print(f\"   MASTER_ADDR = {os.environ.get('MASTER_ADDR')}\")\n",
    "print(f\"   MASTER_PORT = {os.environ.get('MASTER_PORT')}\")\n",
    "print(f\"‚ö†Ô∏è  MUST be IP address, NOT hostname!\")\n",
    "print(f\"‚ÑπÔ∏è  Waiting for Windows worker to connect...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7885a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERIFY IP-only setup (run this BEFORE starting Windows worker)\n",
    "def verify_ip_setup():\n",
    "    \"\"\"Verify that we're using IP addresses, not hostnames\"\"\"\n",
    "    print(\"üîç IP Setup Verification:\")\n",
    "    print(f\"‚úÖ Auto-detected IP: {local_ip}\")\n",
    "    print(f\"‚úÖ MASTER_ADDR will be: {local_ip}\")\n",
    "    \n",
    "    # Check that it's a valid IP format\n",
    "    import re\n",
    "    ip_pattern = r'^(\\d{1,3}\\.){3}\\d{1,3}$'\n",
    "    if re.match(ip_pattern, local_ip):\n",
    "        print(f\"‚úÖ Valid IP format: {local_ip}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Invalid IP format: {local_ip}\")\n",
    "        return False\n",
    "    \n",
    "    # Test if port is available\n",
    "    import socket\n",
    "    try:\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "            s.bind((local_ip, 12355))\n",
    "            print(f\"‚úÖ Port 12355 available on {local_ip}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Port 12355 not available: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run verification\n",
    "verify_ip_setup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411b5af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize distributed process group (run this after Windows worker is ready)\n",
    "def initialize_distributed():\n",
    "    \"\"\"Initialize the distributed environment with FORCED IP (no hostname)\"\"\"\n",
    "    try:\n",
    "        backend = 'gloo'\n",
    "        \n",
    "        # COMPLETELY CLEAR any hostname-related environment variables\n",
    "        hostname_vars = ['HOSTNAME', 'HOST', 'COMPUTERNAME']\n",
    "        for var in hostname_vars:\n",
    "            if var in os.environ:\n",
    "                del os.environ[var]\n",
    "        \n",
    "        # FORCE IP address - multiple methods to ensure no hostname resolution\n",
    "        os.environ['MASTER_ADDR'] = local_ip\n",
    "        os.environ['MASTER_PORT'] = '12355'\n",
    "        os.environ['WORLD_SIZE'] = '2'\n",
    "        os.environ['RANK'] = '0'\n",
    "        \n",
    "        # Clear any problematic gloo settings\n",
    "        gloo_vars = ['GLOO_SOCKET_IFNAME', 'NCCL_SOCKET_IFNAME', 'GLOO_DEVICE_TRANSPORT', 'GLOO_SOCKET_FAMILY']\n",
    "        for var in gloo_vars:\n",
    "            if var in os.environ:\n",
    "                del os.environ[var]\n",
    "        \n",
    "        print(f\"üîß FORCING IP-only connection:\")\n",
    "        print(f\"   MASTER_ADDR = {os.environ['MASTER_ADDR']}\")\n",
    "        print(f\"   NO hostname resolution allowed!\")\n",
    "        \n",
    "        print(f\"üîÑ Initializing distributed training with backend: {backend}\")\n",
    "        \n",
    "        # Method 1: Use environment variables only (more reliable on Mac)\n",
    "        print(f\"üîÑ Using environment variable method...\")\n",
    "        \n",
    "        dist.init_process_group(\n",
    "            backend=backend, \n",
    "            timeout=torch.distributed.default_pg_timeout\n",
    "        )\n",
    "        print(\"‚úÖ Environment method successful!\")\n",
    "        \n",
    "        rank = dist.get_rank()\n",
    "        world_size = dist.get_world_size()\n",
    "        \n",
    "        print(f\"‚úÖ Distributed initialized!\")\n",
    "        print(f\"Rank: {rank}, World size: {world_size}\")\n",
    "        print(\"üñ•Ô∏è  This is the Mac coordinator (Rank 0)\")\n",
    "        \n",
    "        return backend\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to initialize distributed: {e}\")\n",
    "        print(\"üîç Error details:\")\n",
    "        print(f\"   Exception type: {type(e).__name__}\")\n",
    "        print(f\"   Exception message: {str(e)}\")\n",
    "        print(\"\\nüí° Troubleshooting:\")\n",
    "        print(\"1. Ensure Windows worker is running first\")\n",
    "        print(\"2. Check that Windows worker connected to the correct IP\")\n",
    "        print(\"3. Verify firewall settings on both machines\")\n",
    "        return None\n",
    "\n",
    "# Don't run this yet - wait for Windows worker\n",
    "print(\"‚ö†Ô∏è  Ready to run coordinator\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a90a958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# START MAC COORDINATOR - Run this to start listening for Windows worker\n",
    "print(\"üöÄ Starting Mac coordinator...\")\n",
    "backend = initialize_distributed()\n",
    "\n",
    "if backend:\n",
    "    print(\"üéâ SUCCESS! Mac coordinator is running!\")\n",
    "    print(\"‚úÖ Now start the Windows worker - it will connect automatically!\")\n",
    "    print(\"üß™ Ready for distributed operations!\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to start coordinator.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc7011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "from nbdistributed import make_distributed\n",
    "import os\n",
    "import socket\n",
    "\n",
    "# Check if CUDA is available on this machine (Mac)\n",
    "print(f\"CUDA available on Mac: {torch.cuda.is_available()}\")\n",
    "print(f\"Mac will run as CPU coordinator (Rank 0)\")\n",
    "\n",
    "# Check for Mac's Metal GPU support (optional)\n",
    "if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    print(\"Mac Metal Performance Shaders (MPS) available for local operations\")\n",
    "    mac_device = \"mps\"\n",
    "else:\n",
    "    print(\"Using CPU for Mac operations\")\n",
    "    mac_device = \"cpu\"\n",
    "\n",
    "print(f\"Mac device: {mac_device}\")\n",
    "\n",
    "# Get local IP address for network setup\n",
    "hostname = socket.gethostname()\n",
    "local_ip = socket.gethostbyname(hostname)\n",
    "print(f\"Mac IP (Master): {local_ip}\")\n",
    "print(f\"Hostname: {hostname}\")\n",
    "print(\"\\nSetup: Mac (CPU/MPS coordinator) + Windows (CUDA GPU worker)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edf2786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# START MAC COORDINATOR - Run this to start listening for Windows worker\n",
    "print(\"üöÄ Starting Mac coordinator...\")\n",
    "backend = initialize_distributed()\n",
    "\n",
    "if backend:\n",
    "    print(\"üéâ SUCCESS! Mac coordinator is running!\")\n",
    "    print(\"‚úÖ Now start the Windows worker - it will connect automatically!\")\n",
    "    print(\"üß™ Ready for distributed operations!\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to start coordinator.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9fbae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test distributed all_gather operations\n",
    "def test_all_gather_basic():\n",
    "    \"\"\"Test basic all_gather operation across distributed nodes\"\"\"\n",
    "    if not dist.is_initialized():\n",
    "        print(\"‚ùå Distributed not initialized! Call initialize_distributed() first.\")\n",
    "        return None\n",
    "    \n",
    "    rank = dist.get_rank()\n",
    "    world_size = dist.get_world_size()\n",
    "    \n",
    "    # Device allocation based on rank and machine type\n",
    "    if rank == 0:\n",
    "        # Mac coordinator - use CPU or MPS if available\n",
    "        device = torch.device(mac_device)  # \"cpu\" or \"mps\"\n",
    "        print(f\"üñ•Ô∏è  Rank {rank} (Mac): Running on device {device}\")\n",
    "    else:\n",
    "        # Windows worker - should use CUDA\n",
    "        device = torch.device(\"cuda:0\")  # Windows RTX 2080\n",
    "        print(f\"üíª Rank {rank} (Windows): Running on device {device}\")\n",
    "    \n",
    "    # Create tensors with different data for each rank\n",
    "    tensor_list = [torch.zeros(3, dtype=torch.int64) for _ in range(world_size)]\n",
    "    \n",
    "    # Create input tensor with rank-specific data\n",
    "    input_tensor = torch.tensor([rank*100, rank*100+10, rank*100+20], dtype=torch.int64)\n",
    "    \n",
    "    # Move to appropriate device\n",
    "    if rank == 0:\n",
    "        input_tensor = input_tensor.to(device)\n",
    "        tensor_list = [t.to(device) for t in tensor_list]\n",
    "    \n",
    "    print(f\"Rank {rank}: Before all_gather - input: {input_tensor}\")\n",
    "    print(f\"Rank {rank}: Before all_gather - tensor_list: {tensor_list}\")\n",
    "    \n",
    "    # Perform all_gather\n",
    "    dist.all_gather(tensor_list, input_tensor)\n",
    "    \n",
    "    print(f\"Rank {rank}: After all_gather - tensor_list: {tensor_list}\")\n",
    "    \n",
    "    # Synchronize based on device type\n",
    "    if rank == 0 and mac_device == \"mps\":\n",
    "        torch.mps.synchronize()\n",
    "    elif rank != 0:  # Windows with CUDA\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    return tensor_list\n",
    "\n",
    "print(\"üß™ test_all_gather_basic() function ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77d8e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETE INITIALIZATION - Run this cell after Windows worker is ready\n",
    "\n",
    "# Initialize distributed process group\n",
    "def initialize_distributed():\n",
    "    \"\"\"Initialize the distributed environment\"\"\"\n",
    "    try:\n",
    "        # Use 'gloo' backend for CPU-GPU mixed setup (Mac CPU + Windows GPU)\n",
    "        backend = 'gloo'\n",
    "        \n",
    "        print(f\"üîÑ Initializing distributed training with backend: {backend}\")\n",
    "        print(f\"Backend 'gloo' chosen for Mac (CPU/MPS) + Windows (GPU) setup\")\n",
    "        \n",
    "        # Initialize the process group\n",
    "        dist.init_process_group(backend=backend, timeout=torch.distributed.default_pg_timeout)\n",
    "        \n",
    "        rank = dist.get_rank()\n",
    "        world_size = dist.get_world_size()\n",
    "        \n",
    "        print(f\"‚úÖ Distributed initialized!\")\n",
    "        print(f\"Rank: {rank}, World size: {world_size}\")\n",
    "        \n",
    "        if rank == 0:\n",
    "            print(\"üñ•Ô∏è  This is the Mac coordinator (Rank 0)\")\n",
    "        else:\n",
    "            print(f\"üíª This is worker rank {rank}\")\n",
    "        \n",
    "        return backend\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to initialize distributed: {e}\")\n",
    "        print(\"Make sure Windows worker is running first!\")\n",
    "        return None\n",
    "\n",
    "# NOW: Initialize the distributed connection with Windows\n",
    "print(\"üöÄ Attempting to connect to Windows worker...\")\n",
    "backend = initialize_distributed()\n",
    "\n",
    "if backend:\n",
    "    print(\"üéâ SUCCESS! Connected to Windows worker!\")\n",
    "    print(\"üß™ Ready to run distributed operations!\")\n",
    "else:\n",
    "    print(\"‚ùå Connection failed. Check Windows worker status.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57387ec4",
   "metadata": {},
   "source": [
    "## Configure Distributed Setup\n",
    "\n",
    "**Step 1**: Note down your Mac's IP address from above. You'll need this for the Windows machine.\n",
    "\n",
    "**Step 2**: Configure the distributed environment. Update the `WINDOWS_IP` with your Windows laptop's IP address.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fe973d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributed configuration\n",
    "MASTER_ADDR = local_ip  # This Mac will be the master\n",
    "MASTER_PORT = \"12355\"   # Port for communication\n",
    "WORLD_SIZE = 2          # Total number of processes (Mac + Windows)\n",
    "RANK = 0               # This machine's rank (master = 0)\n",
    "\n",
    "# TODO: Update this with your Windows laptop's IP\n",
    "WINDOWS_IP = \"192.168.1.XXX\"  # Replace with actual Windows IP\n",
    "\n",
    "print(f\"Master Address (this Mac): {MASTER_ADDR}\")\n",
    "print(f\"Master Port: {MASTER_PORT}\")\n",
    "print(f\"World Size: {WORLD_SIZE}\")\n",
    "print(f\"This machine's rank: {RANK}\")\n",
    "print(f\"Windows machine IP: {WINDOWS_IP}\")\n",
    "\n",
    "# Set environment variables for distributed training\n",
    "os.environ['MASTER_ADDR'] = MASTER_ADDR\n",
    "os.environ['MASTER_PORT'] = MASTER_PORT\n",
    "os.environ['WORLD_SIZE'] = str(WORLD_SIZE)\n",
    "os.environ['RANK'] = str(RANK)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7719b0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize distributed process group\n",
    "@make_distributed\n",
    "def initialize_distributed():\n",
    "    \"\"\"Initialize the distributed environment\"\"\"\n",
    "    # Use 'gloo' backend for CPU-GPU mixed setup (Mac CPU + Windows GPU)\n",
    "    # 'gloo' supports heterogeneous setups better than 'nccl'\n",
    "    backend = 'gloo'\n",
    "    \n",
    "    print(f\"Initializing distributed training with backend: {backend}\")\n",
    "    print(f\"Backend 'gloo' chosen for Mac (CPU) + Windows (GPU) setup\")\n",
    "    print(f\"Rank: {dist.get_rank()}, World size: {dist.get_world_size()}\")\n",
    "    \n",
    "    rank = dist.get_rank()\n",
    "    if rank == 0:\n",
    "        print(\"This is the Mac coordinator (Rank 0)\")\n",
    "    else:\n",
    "        print(f\"This is worker rank {rank}\")\n",
    "    \n",
    "    return backend\n",
    "\n",
    "# This decorator will handle the distributed setup\n",
    "backend = initialize_distributed()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60595c8d",
   "metadata": {},
   "source": [
    "## All-Gather Operations Test\n",
    "\n",
    "Now let's implement the distributed all_gather operations similar to your original code, but adapted for distributed execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c54828",
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_distributed\n",
    "def test_all_gather_basic():\n",
    "    \"\"\"Test basic all_gather operation across distributed nodes\"\"\"\n",
    "    \n",
    "    rank = dist.get_rank()\n",
    "    world_size = dist.get_world_size()\n",
    "    \n",
    "    # Device allocation based on rank and machine type\n",
    "    if rank == 0:\n",
    "        # Mac coordinator - use CPU or MPS if available\n",
    "        device = torch.device(mac_device)  # \"cpu\" or \"mps\"\n",
    "        print(f\"Rank {rank} (Mac): Running on device {device}\")\n",
    "    else:\n",
    "        # Windows worker - should use CUDA\n",
    "        device = torch.device(f\"cuda:0\")  # Windows RTX 2080\n",
    "        print(f\"Rank {rank} (Windows): Running on device {device}\")\n",
    "    \n",
    "    # Create tensors with different data for each rank\n",
    "    tensor_list = [torch.zeros(3, dtype=torch.int64).to(device) for _ in range(world_size)]\n",
    "    \n",
    "    # Create input tensor with rank-specific data\n",
    "    input_tensor = torch.tensor([rank*100, rank*100+10, rank*100+20], dtype=torch.int64).to(device)\n",
    "    \n",
    "    print(f\"Rank {rank}: Before all_gather - input: {input_tensor}\")\n",
    "    print(f\"Rank {rank}: Before all_gather - tensor_list: {tensor_list}\")\n",
    "    \n",
    "    # Perform all_gather\n",
    "    dist.all_gather(tensor_list, input_tensor)\n",
    "    \n",
    "    print(f\"Rank {rank}: After all_gather - tensor_list: {tensor_list}\")\n",
    "    \n",
    "    # Synchronize based on device type\n",
    "    if rank == 0 and mac_device == \"mps\":\n",
    "        torch.mps.synchronize()\n",
    "    elif rank != 0:  # Windows with CUDA\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    return tensor_list\n",
    "\n",
    "# Run the basic all_gather test\n",
    "result = test_all_gather_basic()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c7b15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_distributed  \n",
    "def test_all_gather_advanced():\n",
    "    \"\"\"Test more complex all_gather operations with timing\"\"\"\n",
    "    import time\n",
    "    \n",
    "    rank = dist.get_rank()\n",
    "    world_size = dist.get_world_size()\n",
    "    \n",
    "    # Device allocation based on rank and machine type\n",
    "    if rank == 0:\n",
    "        # Mac coordinator\n",
    "        device = torch.device(mac_device)\n",
    "        machine_type = \"Mac\"\n",
    "    else:\n",
    "        # Windows worker with RTX 2080\n",
    "        device = torch.device(f\"cuda:0\")\n",
    "        machine_type = \"Windows RTX 2080\"\n",
    "    \n",
    "    print(f\"\\n--- Advanced All-Gather Test ---\")\n",
    "    print(f\"Rank {rank} ({machine_type}): Running on device {device}\")\n",
    "    \n",
    "    # Test with larger tensors\n",
    "    tensor_size = (2, 3)  # 2x3 tensor\n",
    "    tensor_list = [torch.zeros(tensor_size, dtype=torch.int64).to(device) for _ in range(world_size)]\n",
    "    \n",
    "    # Create input tensor with more complex data\n",
    "    input_tensor = torch.tensor([[rank*10, rank*10+1, rank*10+2], \n",
    "                                [rank*10+3, rank*10+4, rank*10+5]], \n",
    "                               dtype=torch.int64).to(device)\n",
    "    \n",
    "    print(f\"Rank {rank}: Input tensor shape: {input_tensor.shape}\")\n",
    "    print(f\"Rank {rank}: Input tensor:\\n{input_tensor}\")\n",
    "    \n",
    "    # Time the operation\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Perform all_gather\n",
    "    dist.all_gather(tensor_list, input_tensor)\n",
    "    \n",
    "    # Synchronize based on device type\n",
    "    if rank == 0 and mac_device == \"mps\":\n",
    "        torch.mps.synchronize()\n",
    "    elif rank != 0:  # Windows with CUDA\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"Rank {rank}: All-gather completed in {end_time - start_time:.4f} seconds\")\n",
    "    print(f\"Rank {rank}: Gathered tensors from all ranks:\")\n",
    "    for i, tensor in enumerate(tensor_list):\n",
    "        machine = \"Mac (CPU/MPS)\" if i == 0 else f\"Windows RTX 2080\"\n",
    "        print(f\"  From rank {i} ({machine}):\\n{tensor}\")\n",
    "    \n",
    "    return tensor_list, end_time - start_time\n",
    "\n",
    "# Run the advanced all_gather test\n",
    "result_advanced, timing = test_all_gather_advanced()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11b4b18",
   "metadata": {},
   "source": [
    "## Windows RTX 2080 Setup Instructions\n",
    "\n",
    "**On your Windows laptop with RTX 2080, create this Python script (`worker.py`):**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import os\n",
    "\n",
    "# Configuration - UPDATE THESE VALUES\n",
    "MASTER_ADDR = \"YOUR_MAC_IP_HERE\"  # Replace with your Mac's IP from above\n",
    "MASTER_PORT = \"12355\"\n",
    "WORLD_SIZE = 2\n",
    "RANK = 1  # Windows machine is rank 1\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['MASTER_ADDR'] = MASTER_ADDR\n",
    "os.environ['MASTER_PORT'] = MASTER_PORT\n",
    "os.environ['WORLD_SIZE'] = str(WORLD_SIZE)\n",
    "os.environ['RANK'] = str(RANK)\n",
    "\n",
    "# Initialize distributed process group\n",
    "dist.init_process_group(backend='gloo')\n",
    "\n",
    "print(f\"Windows worker initialized with CUDA: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Rank: {dist.get_rank()}, World size: {dist.get_world_size()}\")\n",
    "\n",
    "# Keep the worker running and ready for operations\n",
    "try:\n",
    "    print(\"Windows worker ready for distributed operations...\")\n",
    "    # This will keep the process alive for distributed operations\n",
    "    dist.barrier()  # Wait for coordination\n",
    "    print(\"Distributed operations completed!\")\n",
    "finally:\n",
    "    dist.destroy_process_group()\n",
    "```\n",
    "\n",
    "**Steps to run:**\n",
    "1. Install PyTorch with CUDA support on Windows: `pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118`\n",
    "2. Update `MASTER_ADDR` in the script with your Mac's IP\n",
    "3. Run: `python worker.py`\n",
    "4. Then run the cells in this notebook on your Mac\n",
    "\n",
    "**Network requirements:**\n",
    "- Both machines on same network\n",
    "- Port 12355 open for communication\n",
    "- Firewall may need configuration\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
