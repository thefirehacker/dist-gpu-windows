{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0712a1b5",
   "metadata": {},
   "source": [
    "# Distributed PyTorch with nbdistributed\n",
    "\n",
    "## Overview\n",
    "This notebook uses the `nbdistributed` package for interactive distributed computing from Jupyter notebooks.\n",
    "\n",
    "**Note:** Run this notebook on a machine with a GPU (Windows/Linux). It can coordinate with other GPU nodes.\n",
    "\n",
    "## Requirements\n",
    "- `nbdistributed` package\n",
    "- PyTorch with CUDA support\n",
    "- GPU available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfbb12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install nbdistributed torch torchvision --index-url https://download.pytorch.org/whl/cu118\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee83af5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "from nbdistributed import Config, distributed\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    print(\"Running on CPU\")\n",
    "    device = \"cpu\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767f17fb",
   "metadata": {},
   "source": [
    "## Configure Distributed Setup\n",
    "\n",
    "For single GPU, nbdistributed will run locally. For multi-GPU:\n",
    "- Set `MASTER_ADDR` and `MASTER_PORT` environment variables\n",
    "- Run this notebook on multiple machines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbb6f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "@distributed(backend='gloo')\n",
    "def test_all_gather():\n",
    "    \"\"\"Test all_gather operation across GPUs\"\"\"\n",
    "    rank = dist.get_rank()\n",
    "    world_size = dist.get_world_size()\n",
    "    \n",
    "    # Create tensor with rank-specific data\n",
    "    local_tensor = torch.tensor([rank * 10, rank * 10 + 1], dtype=torch.float32)\n",
    "    \n",
    "    # All-gather operation\n",
    "    gathered_tensors = [torch.zeros_like(local_tensor) for _ in range(world_size)]\n",
    "    dist.all_gather(gathered_tensors, local_tensor)\n",
    "    \n",
    "    print(f\"Rank {rank}: Gathered tensors = {gathered_tensors}\")\n",
    "    return gathered_tensors\n",
    "\n",
    "# Run the distributed function\n",
    "result = test_all_gather()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52064dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@distributed(backend='gloo')\n",
    "def test_all_reduce():\n",
    "    \"\"\"Test all_reduce operation - sum across all GPUs\"\"\"\n",
    "    rank = dist.get_rank()\n",
    "    world_size = dist.get_world_size()\n",
    "    \n",
    "    # Each rank contributes its rank number\n",
    "    local_value = torch.tensor([float(rank)], dtype=torch.float32)\n",
    "    \n",
    "    print(f\"Rank {rank}: Local value before reduce = {local_value.item()}\")\n",
    "    \n",
    "    # Sum across all ranks\n",
    "    dist.all_reduce(local_value, op=dist.ReduceOp.SUM)\n",
    "    \n",
    "    print(f\"Rank {rank}: Sum across all ranks = {local_value.item()}\")\n",
    "    return local_value.item()\n",
    "\n",
    "# Run the distributed function\n",
    "sum_result = test_all_reduce()\n",
    "print(f\"\\nFinal result: {sum_result}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
